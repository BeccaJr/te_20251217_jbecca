
# Analytics Engineering Technical Exercise

This repository contains the complete solution for the **Analytics Engineering Technical Exercise**, including analytical SQL queries and a dbt-based data transformation pipeline built on Snowflake.

---

## ğŸ“Œ Project Overview

The objective of this project is to demonstrate practical Analytics Engineering skills, including:

- Writing analytical SQL to answer business questions
- Designing and implementing a dbt transformation pipeline
- Modeling fact tables for analytical use cases
- Applying data quality tests and documentation best practices
- Communicating assumptions and results clearly

The hypothetical business context is a **scooter rental company** operating across multiple U.S. Metropolitan Statistical Areas (MSAs).

---

## ğŸ—‚ï¸ Repository Structure

```
.
â”œâ”€â”€ analysis/                 # Ad-hoc analysis files (if any)
â”œâ”€â”€ data/                     # Optional local data (not used in this exercise)
â”œâ”€â”€ images/                   # Visual assets (charts and dbt docs screenshots)
â”‚   â”œâ”€â”€ provider_share_dc_2020.png
â”‚   â”œâ”€â”€ dbt_docs_lineage.png
â”‚   â”œâ”€â”€ dbt_docs_fct_provider_msa_day.png
â”‚   â””â”€â”€ dbt_docs_fct_company_msa_avg_disp_monthly.png
â”œâ”€â”€ macros/                   # dbt macros (if applicable)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sources/              # Raw data from Snowflake
â”‚   â”œâ”€â”€ staging/              # Staging models (stg_)
â”‚   â”œâ”€â”€ tmp/                  # Temporary / intermediate models (tmp_)
â”‚   â””â”€â”€ marts/                # Final fact models (fct_)
â”œâ”€â”€ snapshots/                # dbt snapshots (not used)
â”œâ”€â”€ tests/                    # Custom tests (if any)
â”œâ”€â”€ dbt_project.yml           # dbt project configuration
â”œâ”€â”€ packages.yml              # dbt package dependencies
â”œâ”€â”€ profiles_temp.yml         # Example dbt profile (no credentials)
â”œâ”€â”€ SUBMISSION.md             # Exercise answers and results
â””â”€â”€ README.md                 # Project documentation (this file)
```

---

## ğŸ§  Data Sources

Raw data is provided in Snowflake under the `TE_RAW.SCOOTERS` schema:

- **SCOOTER_LOCATIONS**  
  Semi-structured (JSON) scooter location data containing:
  - Scooter identifier (`decoded_id`)
  - Provider
  - Latitude / Longitude
  - Timestamp (`extracted_at`)
  - H3 hexagon indexes

- **US_HEX7_MSA**  
  Mapping table between H3 resolution-7 hexagons and U.S. MSAs.

Raw data sources are declared in the models/sources/ directory using dbt source() configurations.
These definitions act as the contract between the raw data layer and the analytics transformation pipeline.

Defining sources explicitly enables:
  - Clear lineage from raw data to analytical models
  - Source-level documentation and metadata visibility in dbt docs
  - Freshness monitoring and data quality validation at the ingestion boundary

---

## ğŸ§± dbt Modeling Approach

The dbt project follows standard Analytics Engineering best practices:

### 1ï¸âƒ£ Staging Models (`stg_`)
- Extract and normalize fields from raw semi-structured JSON
- Apply consistent naming and data types
- Serve as the single entry point to raw data

### 2ï¸âƒ£ Temporary Models (`tmp_`)
- Deduplicate scooter readings using window functions
- Calculate first/last scooter positions per day
- Compute daily displacement metrics

### 3ï¸âƒ£ Fact Models (`fct_`)
Final analytical models answering the business questions:

#### `fct_provider_msa_day`
**Grain:** provider Ã— MSA Ã— day  
**Metric:** count of unique scooters per provider per MSA per day

#### `fct_company_msa_avg_disp_monthly`
**Grain:** provider Ã— MSA Ã— month  
**Metric:** monthly average of daily scooter displacement, calculated as the distance between the first and last scooter position per day

---

## ğŸ§ª Data Quality & Testing

- Primary keys defined for all fact models
- dbt schema tests applied:
  - `unique`
  - `not_null`
- Source freshness checks configured for raw scooter location data

All tests pass successfully using `dbt test`.

---

## ğŸ“Š Analytical SQL Exercise

Part 1 of the exercise includes:

- SQL queries answering business questions about scooter distribution
- Aggregations by MSA, provider, and time
- A 100% stacked column chart showing daily provider scooter share in the Washington, DC MSA

Queries, results, and visualizations are fully documented in **SUBMISSION.md**.

---

## ğŸ“š Documentation

dbt documentation was generated using:

```bash
dbt docs generate
dbt docs serve
```

Screenshots of the dbt documentation and lineage graphs are included in the `images/` directory.

---

## â–¶ï¸ How to Run the Project

1. Install dbt (CLI)
2. Configure your Snowflake profile using `profiles_temp.yml` as a reference
3. Install dependencies:
   ```bash
   dbt deps
   ```
4. Run models:
   ```bash
   dbt run
   ```
5. Run tests:
   ```bash
   dbt test
   ```
6. Generate documentation:
   ```bash
   dbt docs generate
   dbt docs serve
   ```

---

## ğŸ“ Notes & Assumptions

- Analysis is limited to U.S. MSAs using the `US_HEX7_MSA` mapping
- Scooters are assumed to belong to a single MSA per day based on their hex7 location
- Displacement is calculated using Snowflake spatial functions (`ST_DISTANCE`)
- Timestamps are assumed to be in UTC

---

## ğŸ‘¤ Author

**JosÃ© Celso Becca JÃºnior**  
Analytics Engineering Technical Exercise  

---

## âœ… Status

âœ” Project completed  
âœ” All deliverables submitted  
âœ” dbt models, tests, and documentation finalized  
